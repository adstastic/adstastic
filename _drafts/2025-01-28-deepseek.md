---
layout: post
title: DeepSeek implications
tags: ai tech economics
---

There's a lot of talk about the DeepSeek over the past few weeks, and especially in the last 48 hours as it's finally hit mainstream media.
Here I will focus on the implications of what DeepSeek claim, and my confidence in those claims based on available evidence.

## Background
DeepSeek is a small Chinese AI lab building foundation models.
The implications of DeepSeek's work has only just hit the press, but it's been brewing for at least 6 months. The key papers are:
- [DeepSeek V2](https://arxiv.org/pdf/2405.04434), May 2024
- [DeepSeek Coder V2](https://arxiv.org/pdf/2406.11931), June 2024
- [DeepSeek V3](https://arxiv.org/pdf/2412.19437), December 2024
- [DeepSeek R1](https://arxiv.org/pdf/2501.12948), January 2025

## Key points
Here's a breakdown of the key innovations from the provided sources, categorised as requested:

Performance comparable to frontier models
- Coder-V2 matches GPT4-Turbo in code-specific tasks and beats GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math.
- V3 beats other open-source models and matches GPT-4o and Claude-3.5-Sonnet. It is the strongest open-source _base_ model, especially in code and math.
- R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks.

Training
- RL without supervised data:  R1-Zero is trained via large-scale reinforcemenet learning (RL) without supervised fine-tuning (SFT) as a preliminary step, meaning it was never shown examples of chain-of-thought (CoT) i.e. how to reason. It autonomously explored CoT for solving complex problems - the first evidence LLMs learning to reason purely through RL. It also learned to allocate more thinking time by re-evaluating it's approach ("aha moments"), which [scales accuracy](https://openai.com/index/learning-to-reason-with-llms/).
- Multi-stage training: R1-Zero discovered how to reason from scratch, but because of no SFT the output was hard to parse. To fix this, R1 uses a multi-stage training pipeline with fine-tuning on cold-start data, reasoning-oriented RL, and other techniques[^r1-techniques]. 
- Multi-token Prediction: V3 predicts multiple tokens instead of just the next one, improving performance.

^[r1-techniques]: SFT using rejection sampling on the RL checkpoint combined with supervised data from DeepSeek-V3, and a further RL stage.

*   **Architectural innovations for inference**
    *   **Multi-head latent attention (MLA):** First introduced in DeepSeek-V2, MLA uses low-rank key-value joint compression to reduce the KV cache size during inference. MLA achieves better performance than MHA with a significantly smaller KV cache. This architecture was also used in DeepSeek-V3.
    *   **Mixture of Experts (MoE):** DeepSeek-V2 uses the DeepSeekMoE architecture for economical training. This involves fine-grained expert segmentation and shared expert isolation. This architecture was also used in DeepSeek-V3.
    *   **Auxiliary-loss-free load balancing:** DeepSeek-V3 uses an auxiliary-loss-free strategy for load balancing which aims to minimise the adverse impact on model performance.
*   **Hardware efficiency**
    *   **DualPipe:** DeepSeek-V3 utilizes the DualPipe algorithm for efficient pipeline parallelism, which has fewer pipeline bubbles and overlaps computation and communication.
    *   **Custom communication protocol:** DeepSeek-V2 and V3 use custom CUDA kernels for communications and routing algorithms. DeepSeek-V3 uses efficient cross-node all-to-all communication kernels to fully utilise InfiniBand (IB) and NVLink bandwidths.
   *  **PTX optimisations:** DeepSeek's models use PTX level optimisations.
    *   **FP8 mixed precision training:** DeepSeek-V3 supports FP8 mixed precision training for faster training and reduced GPU memory usage.

*   **Open-source**
    *   DeepSeek-V2 model checkpoints are available on GitHub.
    *   DeepSeek-V2-Lite is released for the open-source community.
    *   DeepSeek-Coder-V2 models are released publicly under a permissive license.
     *   DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 are open-sourced.

*   **Cost**
    *   **DeepSeek-V2** saves 42.5% of training costs compared with DeepSeek 67B.
    *   **DeepSeek-V3** requires only 2.788M H800 GPU hours for its full training, costing $5.576M. Pre-training each trillion tokens costs 180K H800 GPU hours.
    *   **DeepSeek-R1** uses DeepSeek-V3 as its base model and has an unspecified cost associated with synthetic data generation, RL, and SFT.

*   **Team size**
    *   The DeepSeek team is noted to be 20 people. Note that the sources do not explicitly state that this is a company wide employee number, so it may represent the research group.


## Counter claims
### Deepseek circumvented the embargo and secretly has lots of H100s which they can't talk about
The hardware optimisations in the V2 and V3 papers are a _lot_ of work to improve the memory bandwidth between their H800s. All that effort doesn't make sense if you have the higher memory bandwidth of H100, and DeepSeek likely would have focused on scaling like the US AI labs, which essentially pay Nvidia for hardware efficiency gains in the form of new GPUs, pre-built racks/clusters, and updates to CUDA.

### This isn't as big a deal as people are making it



## Confidence:
Full confidence:
- Open source: self-evident

High confidence:
- Model performance: Several benchmarks show comparable performance to frontier models, but there is, as always, contention around which the tasks in which beats frontier models. People like to reduce performance down to a binary state, but LLMs performance can very drastically for different tasks. 
- Algo optimisations: Until a full repro is completed, the architectural and training optimisations can't be fully verified, but the paper goes into sufficient technical details that several prominent experts are impressed.

Low confidence:
- Hardware efficiency: The V3 paper only describes extremely low level, apparently impossible to do in CUDA, optimisations. The paper goes into insufficient detail to implement, so requires open-sourcing to verify.
- Cost: Requires a full repro and hardware efficiency components to verify the end-to-end training cost.
